{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# install libraries\n",
    "%pip install --upgrade tiktoken\n",
    "%pip install --upgrade openai\n",
    "%pip install --upgrade python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import time\n",
    "import pprint\n",
    "import tiktoken\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções que serão utilizadas durante os exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome das regiões Azure OpenAI\n",
    "EASTUS = \"eastus\"\n",
    "CANADAEAST = \"canadaeast\"\n",
    "\n",
    "# Nome dos modelos Azure OpenAI criados na subscrição \n",
    "GPT_35_TURBO = \"gpt-35-turbo\"\n",
    "GPT_35_TURBO_16K = \"gpt-35-turbo-16k\"\n",
    "GPT_35_TURBO_INSTRUCT = \"gpt-35-turbo-instruct\"\n",
    "\n",
    "GPT_4 = \"gpt-4\"\n",
    "GPT_4_32K = \"gpt-4-32k\"\n",
    "GPT_4_TURBO = \"gpt-4-turbo\"\n",
    "GPT_4o = \"gpt-4o\"\n",
    "\n",
    "DALL_E_3 = \"dall-e-3\"\n",
    "TEXT_EMBEDDING_ADA_002 = \"text-embedding-ada-002\"\n",
    "TEXT_EMBEDDING_ADA_3_SMALL = \"text-embedding-3-small\"\n",
    "TEXT_EMBEDDING_ADA_3_LARGE = \"text-embedding-3-large\"\n",
    "\n",
    "load_dotenv() # carregar variáveis de ambiente\n",
    "\n",
    "deployments_in_regions = {\"CANADAEAST\": [GPT_35_TURBO, GPT_4, GPT_4_32K, GPT_4_TURBO, TEXT_EMBEDDING_ADA_002, TEXT_EMBEDDING_ADA_3_SMALL],\n",
    "                          \"EASTUS\": [GPT_35_TURBO_16K, GPT_35_TURBO_INSTRUCT, GPT_4o, DALL_E_3, TEXT_EMBEDDING_ADA_002, TEXT_EMBEDDING_ADA_3_LARGE]}\n",
    "\n",
    "\n",
    "# carregar tokenizador para os modelos de linguagem\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Exemplos de system messages\n",
    "papel = {\"bot\": \"Você é um assistente virtual, capaz de responder perguntas, fornecer informações e que sempre responde em formato JSON.\",\n",
    "        \"escritor\": \"Você é um especialista em escrita e redação, capaz de escrever, compreender e analisar textos em português, especialmente textos opinativos e argumentativos.\",\n",
    "        \"legislador\": \"Você é um especialista em legislação, capaz de escrever, interpretar e analisar leis e normas jurídicas em português.\",\n",
    "        \"comediante\": \"Você é um comediante, capaz de criar piadas, contar histórias engraçadas e fazer humor em português.\",\n",
    "        \"desevolvedor\": \"Você é um desenvolvedor de software, capaz de criar, testar e manter programas de computador em qualquer linguagem, mas em especial em Python, HTML e Javascript\",\n",
    "        \"hacker\": \"Você é um hacker, capaz de invadir sistemas de computadores, roubar informações e burlar sistemas de segurança.\",\n",
    "        \"professor\": \"Você é um professor, capaz de ensinar e explicar conceitos, teorias e práticas em qualquer disciplina. Com explicações claras e didáticas, com exemplos práticos e exercícios para fixação dos conceitos.\",\n",
    "        \"instrutor\": \"Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.\",\n",
    "        \"medico\": \"Você é um médico, capaz de diagnosticar, tratar e prevenir doenças em seres humanos e animais.\",\n",
    "        \"advogado\": \"Você é um advogado, capaz de representar clientes em processos judiciais, elaborar contratos e prestar consultoria jurídica.\",\n",
    "        \"padre\": \"Você é um padre, capaz de ministrar sacramentos, celebrar missas e aconselhar fiéis em questões espirituais.\",\n",
    "        \"rabino\": \"Você é um rabino, capaz de ministrar cerimônias religiosas, ensinar a Torá e aconselhar fiéis em questões espirituais.\",\n",
    "        \"pastor\": \"Você é um pastor, capaz de ministrar cultos, pregar a Bíblia e aconselhar fiéis em questões espirituais.\",}\n",
    "\n",
    "\n",
    "\n",
    "# inicialização do cliente Azure OpenAI Canada East\n",
    "client_canadaeast = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_CANADAEAST\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY_CANADAEAST\"),  \n",
    "    api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "# inicialização do cliente Azure OpenAI East US\n",
    "client_eastus = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_EASTUS\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY_EASTUS\"),  \n",
    "    api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "# funcao para retornar o cliente de acordo com a região\n",
    "def get_client(region):\n",
    "    if region == \"canadaeast\":\n",
    "        return client_canadaeast\n",
    "    elif region == \"eastus\":\n",
    "        return client_eastus\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# função para chamar o modelo de linguagem\n",
    "def call_llm(deployment_name, system_message, question, region=\"canadaeast\"):\n",
    "    #verificar se o modelo está disponível na região\n",
    "    if deployment_name not in deployments_in_regions[region.upper()]:\n",
    "        return {\"0.error\": f\"Model {deployment_name} not available in region {region}\", \n",
    "                \"1.This regions has the following models available\": deployments_in_regions[region.upper()], \n",
    "                f\"2.The model {deployment_name} is available in the following regions\": [region for region, models in deployments_in_regions.items() if deployment_name in models]}\n",
    "    \n",
    "    client = get_client(region)\n",
    "    start_time = time.perf_counter()\n",
    "    response = client.chat.completions.create(\n",
    "        model = deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "    )\n",
    "    elapsed_time = time.perf_counter() - start_time # usado para calcular o tempo de execução\n",
    "    resp = response.choices[0].message.content\n",
    "    tokens_completion = num_tokens_from_string(resp, deployment_name) # conta os tokens da resposta\n",
    "    # conta os tokens da mensagem do sistema e da pergunta (prompt)\n",
    "    tokens_prompt = num_tokens_from_string(system_message, deployment_name) + num_tokens_from_string(question, deployment_name)\n",
    "    # formatar a resposta da função\n",
    "    return {\"0.model\": deployment_name, \n",
    "            \"1.elapsed_time\": elapsed_time, \n",
    "            \"2.response\": resp, \n",
    "            \"3.num_tokens_completion\": tokens_completion, \n",
    "            \"4.num_tokens_prompt\": tokens_prompt,\n",
    "            \"5.system_message\": system_message,\n",
    "            \"6.question\": question}\n",
    "\n",
    "# funcao para contar tokens\n",
    "def num_tokens_from_string(texto, model):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(texto))\n",
    "    return num_tokens\n",
    "\n",
    "# função para gerar embeddings\n",
    "def generate_embeddings(text, model, region=\"canadaeast\"):\n",
    "    client = get_client(region)\n",
    "    start_time = time.perf_counter()\n",
    "    embeddings = client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "    return embeddings, elapsed_time\n",
    "\n",
    "# função para executar modelos simultaneamente\n",
    "# models: lista de modelos\n",
    "# system_message: mensagem do sistema\n",
    "# question: pergunta\n",
    "# retorna uma lista com os resultados\n",
    "def execute_simultaneously(models, system_message, question, debug=False, region=\"canadaeast\"):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(call_llm, model, system_message, question, region) for model in models]\n",
    "        results = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            if debug:\n",
    "                print(future.result())\n",
    "            results.append(future.result())\n",
    "    return results\n",
    "\n",
    "def print_result(result, debug=False):\n",
    "    print(\"Elapsed time:\", result[\"1.elapsed_time\"])\n",
    "    print(\"Response:\", result[\"2.response\"])\n",
    "    print(\"Num tokens completion:\", result[\"3.num_tokens_completion\"])\n",
    "    print(\"Num tokens prompt:\", result[\"4.num_tokens_prompt\"])\n",
    "    if debug:\n",
    "        print(\"System message:\", result[\"5.system_message\"])\n",
    "        print(\"Question:\", result[\"6.question\"])\n",
    "        print(\"Model:\", result[\"0.model\"])\n",
    "        print(\"===========================================================\\n\")  \n",
    "\n",
    "#funcao para imprimir resultados\n",
    "def print_results(system_message, question, results, debug=False):\n",
    "    if debug:\n",
    "        print(\"System message:\", system_message)\n",
    "        print(\"Question:\", question)\n",
    "\n",
    "    for result in results:\n",
    "        print(\"Model:\", result[\"0.model\"], \"Elapsed time:\", result[\"1.elapsed_time\"])\n",
    "        print(\"Response:\", result[\"2.response\"])\n",
    "        print(\"Num tokens completion:\", result[\"3.num_tokens_completion\"], \n",
    "              \", Num tokens prompt:\", result[\"4.num_tokens_prompt\"])\n",
    "        print(\"===========================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1.0153741289996105\n",
      "Response: {\n",
      "  \"próximo_horário_livre\": \"12/03/2022 15:00:00\"\n",
      "}\n",
      "Num tokens completion: 26\n",
      "Num tokens prompt: 182\n"
     ]
    }
   ],
   "source": [
    "def sample_agenda():\n",
    "    agenda = '''12/03/2022 12:00:00 Ocupado 12/03/2022 13:00:00 Ocupado 12/03/2022 14:00:00 Ocupado 12/03/2022 15:00:00 Livre 12/03/2022 16:00:00 Ocupado 12/03/2022 17:00:00 Livre\n",
    "        12/03/2022 18:00:00 Ocupado 12/03/2022 19:00:00 Livre 12/03/2022 20:00:00 Ocupado'''\n",
    "    return call_llm(GPT_35_TURBO, papel[\"bot\"], \n",
    "                    f\"Com base nessa agenda, qual o próximo horário livre? {agenda}\", \"canadaeast\")\n",
    "\n",
    "resposta = sample_agenda()\n",
    "print_result(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de diferentes tons na resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta com tom DESPOJADO:\n",
      "('Inteligência artificial, ou IA, é quando as máquinas são programadas para '\n",
      " 'simular a capacidade humana de aprender, raciocinar, tomar decisões e '\n",
      " 'resolver problemas. Isso é feito através de algoritmos e modelos matemáticos '\n",
      " 'que permitem que as máquinas processem grandes quantidades de dados e '\n",
      " 'aprendam com eles, de forma a tomar decisões ou realizar tarefas de forma '\n",
      " 'autônoma. Em resumo, é como se a máquina tivesse um cérebro próprio, capaz '\n",
      " 'de aprender e tomar decisões como um ser humano. Dá para dizer que é uma '\n",
      " 'espécie de \"mindset\" tecnológico, saca?')\n",
      "Resposta com tom FORMAL:\n",
      "('A inteligência artificial é um ramo da ciência da computação que se dedica '\n",
      " 'ao desenvolvimento de sistemas capazes de realizar tarefas que, comumente, '\n",
      " 'exigiriam inteligência humana. Estes sistemas são projetados para aprender, '\n",
      " 'raciocinar, resolver problemas e tomar decisões de forma autônoma, sempre '\n",
      " 'com o objetivo de simular a capacidade cognitiva humana. Dentre as técnicas '\n",
      " 'mais comuns da inteligência artificial, encontram-se o aprendizado de '\n",
      " 'máquina, a visão computacional, o processamento de linguagem natural e a '\n",
      " 'robótica.')\n",
      "Resposta com tom INFORMAL:\n",
      "('Inteligência artificial é tipo um cérebro de metal, saca? É quando a máquina '\n",
      " 'consegue aprender, raciocinar e tomar decisões de forma autônoma, como se '\n",
      " 'fosse um ser inteligente mesmo. Além disso, ela pode processar um monte de '\n",
      " 'dados e reconhecer padrões, fazendo coisas que normalmente precisariam de '\n",
      " 'inteligência humana. É tipo ter um amigo nerd superinteligente que vive '\n",
      " 'dentro de um computador!')\n"
     ]
    }
   ],
   "source": [
    "def sample_system_message_intonation(question):\n",
    "    system_message = {\"despojado\":\"Você é um assitente que sempre responde em tom despojado, de forma didática e em porguguês, apesar de eventualmente citar palavras em inglês.\",\n",
    "                      \"formal\":\"Você é um assistente que sempre responde em tom formal, rebuscado, sem citar palavras em inglês.\",\n",
    "                      \"informal\":\"Você é um assistente que sempre responde em tom informal, de forma didática e em português, com gírias e expressões coloquiais.\"}\n",
    "    \n",
    "\n",
    "    for item in system_message:\n",
    "        print(f\"Resposta com tom {item.upper()}:\")\n",
    "        r = call_llm(GPT_35_TURBO, system_message[item], question, \"canadaeast\")\n",
    "        pprint.pprint(r[\"2.response\"])\n",
    "\n",
    "sample_system_message_intonation(\"O que é inteligência artificial?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de diferença entre modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.model': 'gpt-35-turbo', '1.elapsed_time': 1.3253236370001105, '2.response': 'Um paradoxo é uma declaração que parece contraditória, mas pode ser verdadeira. Um exemplo famoso é o Paradoxo do Mentiroso, que é a afirmação \"Esta frase é falsa\". Se for verdadeira, então é falsa, mas se for falsa, então é verdadeira.', '3.num_tokens_completion': 68, '4.num_tokens_prompt': 63, '5.system_message': 'Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.', '6.question': 'Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.'}\n",
      "{'0.model': 'gpt-4-32k', '1.elapsed_time': 12.598692859000039, '2.response': 'Paradoxo do avô: Imagine um cenário em que viagens no tempo são possíveis. Você volta no tempo e mata seu avô antes que ele conheça sua avó. Como resultado, um dos seus pais (e, consequentemente, você) não deveria existir. Porém, se você não existir, você não poderia voltar no tempo para matar seu avô, o que significa que seu avô nunca morreria. E assim nós temos um paradoxo.', '3.num_tokens_completion': 106, '4.num_tokens_prompt': 63, '5.system_message': 'Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.', '6.question': 'Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.'}\n",
      "{'0.model': 'gpt-4-turbo', '1.elapsed_time': 15.635071446000438, '2.response': 'Paradoxo: Paradoxo do mentiroso.\\n\\nEstrutura:\\n1. Afirmação: \"Esta frase é falsa.\"\\n2. Análise: Se a frase é verdadeira, então o que ela diz é o caso, portanto é falsa. Mas, se é falsa, então ela é verdadeira, pois afirma ser falsa.\\n3. Conclusão: A frase não pode ser nem verdadeira nem falsa sem entrar em contradição, criando um paradoxo.', '3.num_tokens_completion': 107, '4.num_tokens_prompt': 63, '5.system_message': 'Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.', '6.question': 'Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.'}\n",
      "{'0.model': 'gpt-4', '1.elapsed_time': 29.688691088999803, '2.response': 'O Paradoxo do Barbeiro é um célebre paradoxo lógico que foi formulado pelo matemático britânico Bertrand Russell. Ele é estruturado da seguinte maneira:\\n\\nEm uma aldeia, existe um barbeiro que segue uma regra bastante específica: ele barbeia todos que não se barbeiam a si mesmos.\\n\\nO paradoxo surge quando perguntamos: O barbeiro barbeia a si mesmo?\\n\\nSe ele se barbeia, de acordo com a regra ele não deveria, pois ele só barbeia aqueles que não se barbeiam. Mas, se ele não se barbeia, então ele deve se barbear, pois ele barbeia todos aqueles que não se barbeiam. Portanto, seja qual for a resposta, apresenta-se em uma contradição lógica.', '3.num_tokens_completion': 187, '4.num_tokens_prompt': 63, '5.system_message': 'Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.', '6.question': 'Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.'}\n",
      "Model: gpt-35-turbo Elapsed time: 1.3253236370001105\n",
      "Response: Um paradoxo é uma declaração que parece contraditória, mas pode ser verdadeira. Um exemplo famoso é o Paradoxo do Mentiroso, que é a afirmação \"Esta frase é falsa\". Se for verdadeira, então é falsa, mas se for falsa, então é verdadeira.\n",
      "Num tokens completion: 68 , Num tokens prompt: 63\n",
      "===========================================================\n",
      "\n",
      "Model: gpt-4-32k Elapsed time: 12.598692859000039\n",
      "Response: Paradoxo do avô: Imagine um cenário em que viagens no tempo são possíveis. Você volta no tempo e mata seu avô antes que ele conheça sua avó. Como resultado, um dos seus pais (e, consequentemente, você) não deveria existir. Porém, se você não existir, você não poderia voltar no tempo para matar seu avô, o que significa que seu avô nunca morreria. E assim nós temos um paradoxo.\n",
      "Num tokens completion: 106 , Num tokens prompt: 63\n",
      "===========================================================\n",
      "\n",
      "Model: gpt-4-turbo Elapsed time: 15.635071446000438\n",
      "Response: Paradoxo: Paradoxo do mentiroso.\n",
      "\n",
      "Estrutura:\n",
      "1. Afirmação: \"Esta frase é falsa.\"\n",
      "2. Análise: Se a frase é verdadeira, então o que ela diz é o caso, portanto é falsa. Mas, se é falsa, então ela é verdadeira, pois afirma ser falsa.\n",
      "3. Conclusão: A frase não pode ser nem verdadeira nem falsa sem entrar em contradição, criando um paradoxo.\n",
      "Num tokens completion: 107 , Num tokens prompt: 63\n",
      "===========================================================\n",
      "\n",
      "Model: gpt-4 Elapsed time: 29.688691088999803\n",
      "Response: O Paradoxo do Barbeiro é um célebre paradoxo lógico que foi formulado pelo matemático britânico Bertrand Russell. Ele é estruturado da seguinte maneira:\n",
      "\n",
      "Em uma aldeia, existe um barbeiro que segue uma regra bastante específica: ele barbeia todos que não se barbeiam a si mesmos.\n",
      "\n",
      "O paradoxo surge quando perguntamos: O barbeiro barbeia a si mesmo?\n",
      "\n",
      "Se ele se barbeia, de acordo com a regra ele não deveria, pois ele só barbeia aqueles que não se barbeiam. Mas, se ele não se barbeia, então ele deve se barbear, pois ele barbeia todos aqueles que não se barbeiam. Portanto, seja qual for a resposta, apresenta-se em uma contradição lógica.\n",
      "Num tokens completion: 187 , Num tokens prompt: 63\n",
      "===========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_message = papel[\"instrutor\"]\n",
    "question = \"Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.\"\n",
    "models = [GPT_35_TURBO, GPT_4, GPT_4_32K, GPT_4_TURBO]\n",
    "results = execute_simultaneously(models, system_message, question, True)\n",
    "print_results(system_message, question, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bot', 'escritor', 'legislador', 'comediante', 'desevolvedor', 'hacker', 'professor', 'instrutor', 'medico', 'advogado', 'padre', 'rabino', 'pastor'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papel.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E aí, qual o seu prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faça uma pergunta ao modelo, seja criativo!\n",
    "system_message = papel[\"desevolvedor\"]\n",
    "\n",
    "\n",
    "prompt = f''' '''\n",
    "\n",
    "response = call_llm(GPT_4_32K, system_message, prompt)\n",
    "response[\"2.response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_result(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
