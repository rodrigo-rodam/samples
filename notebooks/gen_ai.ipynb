{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# install libraries\n",
    "%pip install --upgrade tiktoken\n",
    "%pip install --upgrade openai\n",
    "%pip install --upgrade python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import time\n",
    "import pprint\n",
    "import tiktoken\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções que serão utilizadas durante os exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodam/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Nome das regiões Azure OpenAI\n",
    "EASTUS = \"eastus\"\n",
    "CANADAEAST = \"canadaeast\"\n",
    "\n",
    "# Nome dos modelos Azure OpenAI criados na subscrição \n",
    "GPT_35_TURBO = \"gpt-35-turbo\"\n",
    "GPT_35_TURBO_16K = \"gpt-35-turbo-16k\"\n",
    "GPT_35_TURBO_INSTRUCT = \"gpt-35-turbo-instruct\"\n",
    "\n",
    "GPT_4 = \"gpt-4\"\n",
    "GPT_4_32K = \"gpt-4-32k\"\n",
    "GPT_4_TURBO = \"gpt-4-turbo\"\n",
    "GPT_4o = \"gpt-4o\"\n",
    "\n",
    "DALL_E_3 = \"dall-e-3\"\n",
    "TEXT_EMBEDDING_ADA_002 = \"text-embedding-ada-002\"\n",
    "TEXT_EMBEDDING_ADA_3_SMALL = \"text-embedding-3-small\"\n",
    "TEXT_EMBEDDING_ADA_3_LARGE = \"text-embedding-3-large\"\n",
    "\n",
    "load_dotenv() # carregar variáveis de ambiente\n",
    "\n",
    "deployments_in_regions = {\"CANADAEAST\": [GPT_35_TURBO, GPT_4, GPT_4_32K, GPT_4_TURBO, TEXT_EMBEDDING_ADA_002, TEXT_EMBEDDING_ADA_3_SMALL],\n",
    "                          \"EASTUS\": [GPT_35_TURBO_16K, GPT_35_TURBO_INSTRUCT, GPT_4o, DALL_E_3, TEXT_EMBEDDING_ADA_002, TEXT_EMBEDDING_ADA_3_LARGE]}\n",
    "\n",
    "\n",
    "# carregar tokenizador para os modelos de linguagem\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Exemplos de system messages\n",
    "papel = {\"bot\": \"Você é um assistente virtual, capaz de responder perguntas, fornecer informações e que sempre responde em formato JSON.\",\n",
    "        \"escritor\": \"Você é um especialista em escrita e redação, capaz de escrever, compreender e analisar textos em português, especialmente textos opinativos e argumentativos.\",\n",
    "        \"legislador\": \"Você é um especialista em legislação, capaz de escrever, interpretar e analisar leis e normas jurídicas em português.\",\n",
    "        \"comediante\": \"Você é um comediante, capaz de criar piadas, contar histórias engraçadas e fazer humor em português.\",\n",
    "        \"desevolvedor\": \"Você é um desenvolvedor de software, capaz de criar, testar e manter programas de computador em qualquer linguagem, mas em especial em Python, HTML e Javascript\",\n",
    "        \"hacker\": \"Você é um hacker, capaz de invadir sistemas de computadores, roubar informações e burlar sistemas de segurança.\",\n",
    "        \"professor\": \"Você é um professor, capaz de ensinar e explicar conceitos, teorias e práticas em qualquer disciplina. Com explicações claras e didáticas, com exemplos práticos e exercícios para fixação dos conceitos.\",\n",
    "        \"instrutor\": \"Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.\",\n",
    "        \"medico\": \"Você é um médico, capaz de diagnosticar, tratar e prevenir doenças em seres humanos e animais.\",\n",
    "        \"advogado\": \"Você é um advogado, capaz de representar clientes em processos judiciais, elaborar contratos e prestar consultoria jurídica.\",\n",
    "        \"padre\": \"Você é um padre, capaz de ministrar sacramentos, celebrar missas e aconselhar fiéis em questões espirituais.\",\n",
    "        \"rabino\": \"Você é um rabino, capaz de ministrar cerimônias religiosas, ensinar a Torá e aconselhar fiéis em questões espirituais.\",\n",
    "        \"pastor\": \"Você é um pastor, capaz de ministrar cultos, pregar a Bíblia e aconselhar fiéis em questões espirituais.\",}\n",
    "\n",
    "\n",
    "\n",
    "# inicialização do cliente Azure OpenAI Canada East\n",
    "client_canadaeast = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_CANADAEAST\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY_CANADAEAST\"),  \n",
    "    api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "# inicialização do cliente Azure OpenAI East US\n",
    "client_eastus = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_EASTUS\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY_EASTUS\"),  \n",
    "    api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "# funcao para retornar o cliente de acordo com a região\n",
    "def get_client(region):\n",
    "    if region == \"canadaeast\":\n",
    "        return client_canadaeast\n",
    "    elif region == \"eastus\":\n",
    "        return client_eastus\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# função para chamar o modelo de linguagem\n",
    "def call_llm(deployment_name, system_message, question, region=\"canadaeast\"):\n",
    "    #verificar se o modelo está disponível na região\n",
    "    if deployment_name not in deployments_in_regions[region.upper()]:\n",
    "        return {\"0.error\": f\"Model {deployment_name} not available in region {region}\", \n",
    "                \"1.This regions has the following models available\": deployments_in_regions[region.upper()], \n",
    "                f\"2.The model {deployment_name} is available in the following regions\": [region for region, models in deployments_in_regions.items() if deployment_name in models]}\n",
    "    \n",
    "    client = get_client(region)\n",
    "    start_time = time.perf_counter()\n",
    "    response = client.chat.completions.create(\n",
    "        model = deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "    )\n",
    "    elapsed_time = time.perf_counter() - start_time # usado para calcular o tempo de execução\n",
    "    resp = response.choices[0].message.content\n",
    "    tokens_completion = num_tokens_from_string(resp, deployment_name) # conta os tokens da resposta\n",
    "    # conta os tokens da mensagem do sistema e da pergunta (prompt)\n",
    "    tokens_prompt = num_tokens_from_string(system_message, deployment_name) + num_tokens_from_string(question, deployment_name)\n",
    "    # formatar a resposta da função\n",
    "    return {\"0.model\": deployment_name, \n",
    "            \"1.elapsed_time\": elapsed_time, \n",
    "            \"2.response\": resp, \n",
    "            \"3.num_tokens_completion\": tokens_completion, \n",
    "            \"4.num_tokens_prompt\": tokens_prompt,\n",
    "            \"5.system_message\": system_message,\n",
    "            \"6.question\": question}\n",
    "\n",
    "# funcao para contar tokens\n",
    "def num_tokens_from_string(texto, model):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(texto))\n",
    "    return num_tokens\n",
    "\n",
    "# função para gerar embeddings\n",
    "def generate_embeddings(text, model, region=\"canadaeast\"):\n",
    "    client = get_client(region)\n",
    "    start_time = time.perf_counter()\n",
    "    embeddings = client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "    return embeddings, elapsed_time\n",
    "\n",
    "# função para executar modelos simultaneamente\n",
    "# models: lista de modelos\n",
    "# system_message: mensagem do sistema\n",
    "# question: pergunta\n",
    "# retorna uma lista com os resultados\n",
    "def execute_simultaneously(models, system_message, question, debug=False, region=\"canadaeast\"):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(call_llm, model, system_message, question, region) for model in models]\n",
    "        results = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            if debug:\n",
    "                print(future.result())\n",
    "            results.append(future.result())\n",
    "    return results\n",
    "\n",
    "#funcao para imprimir resultados\n",
    "def print_results(system_message, question, results):\n",
    "    print(\"System message:\", system_message)\n",
    "    print(\"Question:\", question)  \n",
    "    for result in results:\n",
    "        print(\"Model:\", result[\"0.model\"])\n",
    "        print(\"Elapsed time:\", result[\"1.elapsed_time\"])\n",
    "        print(\"Response:\", result[\"2.response\"])\n",
    "        print(\"Num tokens completion:\", result[\"3.num_tokens_completion\"])\n",
    "        print(\"Num tokens prompt:\", result[\"4.num_tokens_prompt\"])\n",
    "        print(\"===========================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.model': 'gpt-4',\n",
      " '1.elapsed_time': 2.0809412400000156,\n",
      " '2.response': '{\\n  \"próximo_horário_livre\": \"12/03/2022 15:00:00\"\\n}',\n",
      " '3.num_tokens_completion': 26,\n",
      " '4.num_tokens_prompt': 182,\n",
      " '5.system_message': 'Você é um assistente virtual, capaz de responder '\n",
      "                     'perguntas, fornecer informações e que sempre responde em '\n",
      "                     'formato JSON.',\n",
      " '6.question': 'Com base nessa agenda, qual o próximo horário livre? '\n",
      "               '12/03/2022 12:00:00 Ocupado 12/03/2022 13:00:00 Ocupado '\n",
      "               '12/03/2022 14:00:00 Ocupado 12/03/2022 15:00:00 Livre '\n",
      "               '12/03/2022 16:00:00 Ocupado 12/03/2022 17:00:00 Livre\\n'\n",
      "               '        12/03/2022 18:00:00 Ocupado 12/03/2022 19:00:00 Livre '\n",
      "               '12/03/2022 20:00:00 Ocupado'}\n"
     ]
    }
   ],
   "source": [
    "def sample_agenda():\n",
    "    agenda = '''12/03/2022 12:00:00 Ocupado 12/03/2022 13:00:00 Ocupado 12/03/2022 14:00:00 Ocupado 12/03/2022 15:00:00 Livre 12/03/2022 16:00:00 Ocupado 12/03/2022 17:00:00 Livre\n",
    "        12/03/2022 18:00:00 Ocupado 12/03/2022 19:00:00 Livre 12/03/2022 20:00:00 Ocupado'''\n",
    "    return call_llm(GPT_4, papel[\"bot\"], \n",
    "                    f\"Com base nessa agenda, qual o próximo horário livre? {agenda}\", \"canadaeast\")\n",
    "\n",
    "resposta = sample_agenda()\n",
    "pprint.pprint(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de diferentes tons na resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta com tom DESPOJADO:\n",
      "('Inteligência artificial (IA) é basicamente a capacidade de um computador ou '\n",
      " 'máquina realizar tarefas que normalmente exigiriam inteligência humana. Isso '\n",
      " 'inclui coisas como reconhecimento de padrões, aprendizado, raciocínio, '\n",
      " 'resolução de problemas e tomada de decisões. Em outras palavras, a IA '\n",
      " 'permite que as máquinas ajam de forma inteligente, aprendam com a '\n",
      " 'experiência e se adaptem a novas situações. O objetivo é criar sistemas que '\n",
      " 'possam imitar o pensamento humano e executar tarefas de forma autônoma. Na '\n",
      " 'prática, a IA está por trás de coisas como assistentes virtuais, carros '\n",
      " 'autônomos, sistemas de recomendação e muito mais.')\n",
      "Resposta com tom FORMAL:\n",
      "('A inteligência artificial é um ramo da ciência da computação que se dedica '\n",
      " 'ao desenvolvimento de algoritmos e sistemas capazes de realizar tarefas que, '\n",
      " 'quando executadas por seres humanos, requerem inteligência. Estas tarefas '\n",
      " 'incluem raciocínio, aprendizado, percepção, resolução de problemas e '\n",
      " 'comunicação. A inteligência artificial busca criar máquinas e softwares '\n",
      " 'capazes de simular, de forma autônoma, a capacidade humana de pensar e tomar '\n",
      " 'decisões.')\n",
      "Resposta com tom INFORMAL:\n",
      "('Opa, inteligência artificial é tipo uma tecnologia que permite que as '\n",
      " 'máquinas simulem a inteligência humana. Ela consegue aprender e tomar '\n",
      " 'decisões baseadas em dados, sem precisar da intervenção direta de humanos o '\n",
      " 'tempo todo. É tipo um cérebro eletrônico que pode resolver problemas, '\n",
      " 'reconhecer padrões, tomar decisões, tudo isso sem precisar de um ser humano '\n",
      " 'mexendo os pauzinhos o tempo todo, sacou?')\n"
     ]
    }
   ],
   "source": [
    "def sample_system_message_intonation(question):\n",
    "    system_message = {\"despojado\":\"Você é um assitente que sempre responde em tom despojado, de forma didática e em porguguês, apesar de eventualmente citar palavras em inglês.\",\n",
    "                      \"formal\":\"Você é um assistente que sempre responde em tom formal, rebuscado, sem citar palavras em inglês.\",\n",
    "                      \"informal\":\"Você é um assistente que sempre responde em tom informal, de forma didática e em português, com gírias e expressões coloquiais.\"}\n",
    "    \n",
    "\n",
    "    for item in system_message:\n",
    "        print(f\"Resposta com tom {item.upper()}:\")\n",
    "        r = call_llm(GPT_35_TURBO, system_message[item], question, \"canadaeast\")\n",
    "        pprint.pprint(r[\"2.response\"])\n",
    "\n",
    "sample_system_message_intonation(\"O que é inteligência artificial?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de diferença entre modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.model': 'gpt-35-turbo', '1.elapsed_time': 2.185572154000056, '2.response': 'Um paradoxo interessante é o Paradoxo do Barbeiro, formulado por Bertrand Russell para questionar a consistência da teoria dos conjuntos de Cantor. Ele envolve a ideia de um barbeiro que barbeia todos os homens da aldeia que não barbeiam a si mesmos, e a pergunta é se o barbeiro barbeia a si mesmo. Isso cria uma contradição lógica, pois se o barbeiro se barbeia, ele não deveria barbear a si mesmo, mas se ele não se barbeia, ele deveria barbear a si mesmo. Isso desafia a lógica e levanta questões sobre a natureza dos conjuntos e da auto-referência.', '3.num_tokens_completion': 164, '4.num_tokens_prompt': 63, '5.system_message': 'Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.', '6.question': 'Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.'}\n",
      "{'0.model': 'gpt-4-turbo', '1.elapsed_time': 5.796774015999745, '2.response': 'Paradoxo do barbeiro: Existe uma vila onde há um barbeiro que faz a barba de todos os homens que não se barbeiam sozinhos. O paradoxo surge quando questionamos: O barbeiro se barbeia? Se sim, ele não deveria, pois ele só barbeia aqueles que não se barbeiam. Se não, ele deve barbear-se, pois barbeia todos que não se barbeiam. Isso cria um ciclo sem fim, sem uma resposta lógica, porque qualquer opção leva a uma contradição.', '3.num_tokens_completion': 128, '4.num_tokens_prompt': 63, '5.system_message': 'Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.', '6.question': 'Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.'}\n",
      "{'0.model': 'gpt-4-32k', '1.elapsed_time': 14.039401334000104, '2.response': 'Paradoxo do gato de Schrodinger: Um gato é colocado numa caixa fechada com um dispositivo que tem 50% de chance de liberar veneno. Até que abramos a caixa, teoria quântica sugere que o gato está simultaneamente vivo e morto — um estado chamado superposição. No entanto, quando observamos o gato (abrindo a caixa), ele aparece apenas num estado definitivo: ou está vivo, ou está morto, nunca ambos. Isso contradiz nossa experiência do mundo, onde tudo deve estar ou numa condição ou outra, nunca ambas ao mesmo tempo.', '3.num_tokens_completion': 144, '4.num_tokens_prompt': 63, '5.system_message': 'Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.', '6.question': 'Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.'}\n",
      "{'0.model': 'gpt-4', '1.elapsed_time': 15.54011224099986, '2.response': 'Paradoxo do gato de Schrödinger: Imagine que um gato seja colocado em uma caixa com uma substância radioativa que tem 50% de chances de emitir uma partícula em uma hora. Essa partícula irá acionar um dispositivo que libera veneno e mata o gato. Segundo a interpretação da mecânica quântica, após uma hora, antes de abrirmos a caixa, o gato estará simultaneamente vivo e morto. Só depois de abrirmos a caixa e observarmos, o gato será ou vivo ou morto. Isso é paradoxal à nossa compreensão de que o gato só pode estar em um dos estados (vivo ou morto) em um dado momento.', '3.num_tokens_completion': 175, '4.num_tokens_prompt': 63, '5.system_message': 'Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.', '6.question': 'Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.'}\n",
      "System message: Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.\n",
      "Question: Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.\n",
      "Model: gpt-35-turbo\n",
      "Elapsed time: 2.185572154000056\n",
      "Response: Um paradoxo interessante é o Paradoxo do Barbeiro, formulado por Bertrand Russell para questionar a consistência da teoria dos conjuntos de Cantor. Ele envolve a ideia de um barbeiro que barbeia todos os homens da aldeia que não barbeiam a si mesmos, e a pergunta é se o barbeiro barbeia a si mesmo. Isso cria uma contradição lógica, pois se o barbeiro se barbeia, ele não deveria barbear a si mesmo, mas se ele não se barbeia, ele deveria barbear a si mesmo. Isso desafia a lógica e levanta questões sobre a natureza dos conjuntos e da auto-referência.\n",
      "Num tokens completion: 164\n",
      "Num tokens prompt: 63\n",
      "===========================================================\n",
      "\n",
      "Model: gpt-4-turbo\n",
      "Elapsed time: 5.796774015999745\n",
      "Response: Paradoxo do barbeiro: Existe uma vila onde há um barbeiro que faz a barba de todos os homens que não se barbeiam sozinhos. O paradoxo surge quando questionamos: O barbeiro se barbeia? Se sim, ele não deveria, pois ele só barbeia aqueles que não se barbeiam. Se não, ele deve barbear-se, pois barbeia todos que não se barbeiam. Isso cria um ciclo sem fim, sem uma resposta lógica, porque qualquer opção leva a uma contradição.\n",
      "Num tokens completion: 128\n",
      "Num tokens prompt: 63\n",
      "===========================================================\n",
      "\n",
      "Model: gpt-4-32k\n",
      "Elapsed time: 14.039401334000104\n",
      "Response: Paradoxo do gato de Schrodinger: Um gato é colocado numa caixa fechada com um dispositivo que tem 50% de chance de liberar veneno. Até que abramos a caixa, teoria quântica sugere que o gato está simultaneamente vivo e morto — um estado chamado superposição. No entanto, quando observamos o gato (abrindo a caixa), ele aparece apenas num estado definitivo: ou está vivo, ou está morto, nunca ambos. Isso contradiz nossa experiência do mundo, onde tudo deve estar ou numa condição ou outra, nunca ambas ao mesmo tempo.\n",
      "Num tokens completion: 144\n",
      "Num tokens prompt: 63\n",
      "===========================================================\n",
      "\n",
      "Model: gpt-4\n",
      "Elapsed time: 15.54011224099986\n",
      "Response: Paradoxo do gato de Schrödinger: Imagine que um gato seja colocado em uma caixa com uma substância radioativa que tem 50% de chances de emitir uma partícula em uma hora. Essa partícula irá acionar um dispositivo que libera veneno e mata o gato. Segundo a interpretação da mecânica quântica, após uma hora, antes de abrirmos a caixa, o gato estará simultaneamente vivo e morto. Só depois de abrirmos a caixa e observarmos, o gato será ou vivo ou morto. Isso é paradoxal à nossa compreensão de que o gato só pode estar em um dos estados (vivo ou morto) em um dado momento.\n",
      "Num tokens completion: 175\n",
      "Num tokens prompt: 63\n",
      "===========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_message = papel[\"instrutor\"]\n",
    "question = \"Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.\"\n",
    "models = [GPT_35_TURBO, GPT_4, GPT_4_32K, GPT_4_TURBO]\n",
    "results = execute_simultaneously(models, system_message, question, True)\n",
    "print_results(system_message, question, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bot', 'escritor', 'legislador', 'comediante', 'desevolvedor', 'hacker', 'professor', 'instrutor', 'medico', 'advogado', 'padre', 'rabino', 'pastor'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papel.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E aí, qual o seu prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faça uma pergunta ao modelo, seja criativo!\n",
    "system_message = papel[\"desevolvedor\"]\n",
    "\n",
    "prompt = ''' '''\n",
    "\n",
    "response = call_llm(GPT_4_32K, system_message, prompt)\n",
    "response[\"2.response\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
