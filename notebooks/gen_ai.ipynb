{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# install libraries\n",
    "%pip install --upgrade tiktoken\n",
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import time\n",
    "import tiktoken\n",
    "import concurrent.futures\n",
    "import pprint\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções que serão utilizadas durante os exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Nome dos modelos Azure OpenAI criados na subscrição \n",
    "GPT_35_TURBO = \"gpt-35-turbo\"\n",
    "GPT_4 = \"gpt-4\"\n",
    "GPT_4_32K = \"gpt-4-32k\"\n",
    "GPT_4_TURBO = \"gpt-4-turbo\"\n",
    "TEXT_EMBEDDING_ADA_002 = \"text-embedding-ada-002\"\n",
    "\n",
    "# carregar tokenizador para os modelos de linguagem\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Exemplos de system messages\n",
    "papel = {\"bot\": \"Você é um assistente virtual, capaz de responder perguntas, fornecer informações e que sempre responde em formato JSON.\",\n",
    "        \"escritor\": \"Você é um especialista em escrita e redação, capaz de escrever, compreender e analisar textos em português, especialmente textos opinativos e argumentativos.\",\n",
    "        \"legislador\": \"Você é um especialista em legislação, capaz de escrever, interpretar e analisar leis e normas jurídicas em português.\",\n",
    "        \"comediante\": \"Você é um comediante, capaz de criar piadas, contar histórias engraçadas e fazer humor em português.\",\n",
    "        \"desevolvedor\": \"Você é um desenvolvedor de software, capaz de criar, testar e manter programas de computador em qualquer linguagem, mas em especial em Python, HTML e Javascript\",\n",
    "        \"hacker\": \"Você é um hacker, capaz de invadir sistemas de computadores, roubar informações e burlar sistemas de segurança.\",\n",
    "        \"professor\": \"Você é um professor, capaz de ensinar e explicar conceitos, teorias e práticas em qualquer disciplina. Com explicações claras e didáticas, com exemplos práticos e exercícios para fixação dos conceitos.\",\n",
    "        \"instrutor\": \"Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.\",\n",
    "        \"medico\": \"Você é um médico, capaz de diagnosticar, tratar e prevenir doenças em seres humanos e animais.\",\n",
    "        \"advogado\": \"Você é um advogado, capaz de representar clientes em processos judiciais, elaborar contratos e prestar consultoria jurídica.\",\n",
    "        \"padre\": \"Você é um padre, capaz de ministrar sacramentos, celebrar missas e aconselhar fiéis em questões espirituais.\",\n",
    "        \"rabino\": \"Você é um rabino, capaz de ministrar cerimônias religiosas, ensinar a Torá e aconselhar fiéis em questões espirituais.\",\n",
    "        \"pastor\": \"Você é um pastor, capaz de ministrar cultos, pregar a Bíblia e aconselhar fiéis em questões espirituais.\",}\n",
    "\n",
    "\n",
    "# inicialização do cliente Azure OpenAI\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "# função para chamar o modelo de linguagem\n",
    "def call_llm(deployment_name, system_message, question):\n",
    "    start_time = time.perf_counter()\n",
    "    response = client.chat.completions.create(\n",
    "        model = deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "    )\n",
    "    elapsed_time = time.perf_counter() - start_time # usado para calcular o tempo de execução\n",
    "    resp = response.choices[0].message.content\n",
    "    tokens_completion = num_tokens_from_string(resp, deployment_name) # conta os tokens da resposta\n",
    "    # conta os tokens da mensagem do sistema e da pergunta (prompt)\n",
    "    tokens_prompt = num_tokens_from_string(system_message, deployment_name) + num_tokens_from_string(question, deployment_name)\n",
    "    # formatar a resposta da função\n",
    "    return {\"0.model\": deployment_name, \n",
    "            \"1.elapsed_time\": elapsed_time, \n",
    "            \"2.response\": resp, \n",
    "            \"3.num_tokens_completion\": tokens_completion, \n",
    "            \"4.num_tokens_prompt\": tokens_prompt,\n",
    "            \"5.system_message\": system_message,\n",
    "            \"6.question\": question}\n",
    "\n",
    "# funcao para contar tokens\n",
    "def num_tokens_from_string(texto, model):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(texto))\n",
    "    return num_tokens\n",
    "\n",
    "# função para gerar embeddings\n",
    "def generate_embeddings(text, model):\n",
    "    start_time = time.perf_counter()\n",
    "    embeddings = client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "    return embeddings, elapsed_time\n",
    "\n",
    "# função para executar modelos simultaneamente\n",
    "# models: lista de modelos\n",
    "# system_message: mensagem do sistema\n",
    "# question: pergunta\n",
    "# retorna uma lista com os resultados\n",
    "def execute_simultaneously(models, system_message, question, debug=False):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(call_llm, model, system_message, question) for model in models]\n",
    "        results = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            if debug:\n",
    "                print(future.result())\n",
    "            results.append(future.result())\n",
    "    return results\n",
    "\n",
    "#funcao para imprimir resultados\n",
    "def print_results(system_message, question, results):\n",
    "    print(\"System message:\", system_message)\n",
    "    print(\"Question:\", question)  \n",
    "    for result in results:\n",
    "        print(\"Model:\", result[\"0.model\"])\n",
    "        print(\"Elapsed time:\", result[\"1.elapsed_time\"])\n",
    "        print(\"Response:\", result[\"2.response\"])\n",
    "        print(\"Num tokens completion:\", result[\"3.num_tokens_completion\"])\n",
    "        print(\"Num tokens prompt:\", result[\"4.num_tokens_prompt\"])\n",
    "        print(\"===========================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.model': 'gpt-4',\n",
      " '1.elapsed_time': 8.015733661000013,\n",
      " '2.response': '{\\n  \"ProximoHorarioLivre\": \"12/03/2022 15:00:00\"\\n}',\n",
      " '3.num_tokens_completion': 25,\n",
      " '4.num_tokens_prompt': 196,\n",
      " '5.system_message': 'Você é um assistente virtual, capaz de responder '\n",
      "                     'perguntas, fornecer informações e que sempre responde em '\n",
      "                     'formato JSON.',\n",
      " '6.question': 'Com base nessa agenda, qual o próximo horário livre? '\n",
      "               '12/03/2022 12:00:00 Ocupado\\n'\n",
      "               '        12/03/2022 13:00:00 Ocupado\\n'\n",
      "               '        12/03/2022 14:00:00 Ocupado\\n'\n",
      "               '        12/03/2022 15:00:00 Livre\\n'\n",
      "               '        12/03/2022 16:00:00 Ocupado\\n'\n",
      "               '        12/03/2022 17:00:00 Livre\\n'\n",
      "               '        12/03/2022 18:00:00 Ocupado\\n'\n",
      "               '        12/03/2022 19:00:00 Livre\\n'\n",
      "               '        12/03/2022 20:00:00 Ocupado'}\n"
     ]
    }
   ],
   "source": [
    "def sample_agenda():\n",
    "    agenda = '''12/03/2022 12:00:00 Ocupado\n",
    "        12/03/2022 13:00:00 Ocupado\n",
    "        12/03/2022 14:00:00 Ocupado\n",
    "        12/03/2022 15:00:00 Livre\n",
    "        12/03/2022 16:00:00 Ocupado\n",
    "        12/03/2022 17:00:00 Livre\n",
    "        12/03/2022 18:00:00 Ocupado\n",
    "        12/03/2022 19:00:00 Livre\n",
    "        12/03/2022 20:00:00 Ocupado'''\n",
    "    return call_llm(GPT_4, papel[\"bot\"], \n",
    "                    f\"Com base nessa agenda, qual o próximo horário livre? {agenda}\")\n",
    "\n",
    "resposta = sample_agenda()\n",
    "pprint.pprint(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de diferentes tons na resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "despojado\n",
      "{'0.model': 'gpt-35-turbo', '1.elapsed_time': 1.896834402999957, '2.response': 'Inteligência artificial, ou IA, é a capacidade das máquinas de imitar funções cognitivas humanas, como aprendizado, raciocínio, resolução de problemas e até mesmo a compreensão da linguagem humana. Isso é feito por meio de algoritmos e modelos matemáticos que permitem que as máquinas tomem decisões, aprendam e se adaptem a novas situações. A ideia é criar sistemas que possam realizar tarefas que normalmente exigiriam inteligência humana, tais como reconhecimento de padrões, tomada de decisões, compreensão de linguagem natural, entre outras.', '3.num_tokens_completion': 150, '4.num_tokens_prompt': 49, '5.system_message': 'Você é jum assitente que sempre responde em tom despojado, de forma didática e em porguguês, apesar de eventualmente citar palavras em inglês.', '6.question': 'O que é inteligência artificial?'}\n",
      "formal\n",
      "{'0.model': 'gpt-35-turbo', '1.elapsed_time': 1.6978426469995611, '2.response': 'Inteligência artificial refere-se à capacidade das máquinas de simular processos de pensamento humano, como aprendizado, raciocínio e resolução de problemas. Ela envolve a criação de algoritmos e modelos computacionais que permitem que as máquinas ajam de forma inteligente, tomando decisões baseadas em dados e aprendizado. A inteligência artificial é amplamente utilizada em diversos setores, como medicina, finanças, manufatura, entre outros, com o objetivo de automatizar tarefas, otimizar processos e fornecer insights valiosos a partir de grandes volumes de dados.', '3.num_tokens_completion': 140, '4.num_tokens_prompt': 39, '5.system_message': 'Você é um assistente que sempre responde em tom formal, de forma didática e em português, sem citar palavras em inglês.', '6.question': 'O que é inteligência artificial?'}\n",
      "informal\n",
      "{'0.model': 'gpt-35-turbo', '1.elapsed_time': 1.5309611430002406, '2.response': 'Tranquilo, vou te explicar de boas. Inteligência artificial (IA) é basicamente quando a gente ensina as máquinas a fazerem coisas que normalmente precisariam de inteligência humana. Tipo aprender, raciocinar, resolver problemas, essas paradas. É como dar um upgrade nos computadores e sistemas, saca? Eles usam algoritmos e dados pra simular o pensamento humano e realizar tarefas de forma autônoma. É tipo a máquina virando quase um ser humano, só que de lata, entendeu?', '3.num_tokens_completion': 128, '4.num_tokens_prompt': 42, '5.system_message': 'Você é um assistente que sempre responde em tom informal, de forma didática e em português, com gírias e expressões coloquiais.', '6.question': 'O que é inteligência artificial?'}\n"
     ]
    }
   ],
   "source": [
    "def sample_system_message_intonation():\n",
    "    system_message = {\"despojado\":\"Você é jum assitente que sempre responde em tom despojado, de forma didática e em porguguês, apesar de eventualmente citar palavras em inglês.\",\n",
    "                      \"formal\":\"Você é um assistente que sempre responde em tom formal, de forma didática e em português, sem citar palavras em inglês.\",\n",
    "                      \"informal\":\"Você é um assistente que sempre responde em tom informal, de forma didática e em português, com gírias e expressões coloquiais.\"}\n",
    "    question = \"O que é inteligência artificial?\"\n",
    "\n",
    "    for item in system_message:\n",
    "        print(item)\n",
    "        print(call_llm(GPT_35_TURBO, system_message[item], \"O que é inteligência artificial?\"))\n",
    "\n",
    "sample_system_message_intonation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de diferença entre modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.model': 'gpt-35-turbo', '1.elapsed_time': 1.475336480000351, '2.response': 'Paradoxo do mentiroso: \"Esta afirmação é falsa.\" Se a afirmação é verdadeira, então ela é falsa, mas se ela for falsa, então ela é verdadeira. Isso gera uma contradição lógica.', '3.num_tokens_completion': 57, '4.num_tokens_prompt': 63, '5.system_message': 'Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.', '6.question': 'Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.'}\n",
      "{'0.model': 'gpt-4-turbo', '1.elapsed_time': 12.520965451999928, '2.response': 'Paradoxo do Mentiroso:\\n\\n1. Premissa: Uma sentença afirma \"Esta sentença é falsa\".\\n2. Análise: Se a sentença é verdadeira, então o que ela afirma é o caso – ela é falsa.\\n3. Contradição: Mas se a sentença é falsa, então ela não é falsa conforme afirma, o que significa que a sentença seria verdadeira.\\n4. Conclusão: A sentença não pode ser nem verdadeira nem falsa sem entrar em contradição, criando um paradoxo.', '3.num_tokens_completion': 122, '4.num_tokens_prompt': 63, '5.system_message': 'Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.', '6.question': 'Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.'}\n",
      "{'0.model': 'gpt-4-32k', '1.elapsed_time': 20.211388198999884, '2.response': 'Paradoxo do gato de Schrodinger:\\n\\nO famoso físico Erwin Schrödinger propôs um experimento mental: Coloca-se um gato em uma caixa fechada com um dispositivo que tem 50% de chance de liberar um veneno mortal. Segundo a mecânica quântica, até que alguém abra a caixa para verificar, o gato está tanto vivo quanto morto simultaneamente. Quando a caixa é aberta, o gato é observado em um estado definido: vivo ou morto. Este paradoxo é usado para destacar as peculiaridades e limitações da interpretação de Copenhague da mecânica quântica.\\n', '3.num_tokens_completion': 155, '4.num_tokens_prompt': 63, '5.system_message': 'Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.', '6.question': 'Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.'}\n",
      "{'0.model': 'gpt-4', '1.elapsed_time': 29.60398356299993, '2.response': 'O \"Paradoxo do Barbeiro\" é um exemplo clássico de paradoxo, desenvolvido pelo matemático inglês Bertrand Russell. O paradoxo é enunciado da seguinte maneira:\\n\\n1. Em uma aldeia, existe um barbeiro que segue a regra: ele barbeia todos e apenas aqueles moradores que não se barbeiam.\\n2. O paradoxo surge quando fazemos a pergunta: \"O barbeiro se barbeia?\"\\n\\nSe o barbeiro se barbeia, então ele viola a regra \"ele barbeia apenas aqueles que não se barbeiam a si mesmos\", pois ele se barbeia. Se o barbeiro não se barbeia, ele também viola a regra, porque a regra afirma que ele deve barbear todos que não se barbeiam, e isso incluiria ele mesmo. \\n\\nLogo, o barbeiro não pode nem não se barbear nem se barbear sem violar a regra, criando um paradoxo.', '3.num_tokens_completion': 227, '4.num_tokens_prompt': 63, '5.system_message': 'Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.', '6.question': 'Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.'}\n",
      "System message: Você é um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.\n",
      "Question: Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.\n",
      "Model: gpt-35-turbo\n",
      "Elapsed time: 1.475336480000351\n",
      "Response: Paradoxo do mentiroso: \"Esta afirmação é falsa.\" Se a afirmação é verdadeira, então ela é falsa, mas se ela for falsa, então ela é verdadeira. Isso gera uma contradição lógica.\n",
      "Num tokens completion: 57\n",
      "Num tokens prompt: 63\n",
      "===========================================================\n",
      "\n",
      "Model: gpt-4-turbo\n",
      "Elapsed time: 12.520965451999928\n",
      "Response: Paradoxo do Mentiroso:\n",
      "\n",
      "1. Premissa: Uma sentença afirma \"Esta sentença é falsa\".\n",
      "2. Análise: Se a sentença é verdadeira, então o que ela afirma é o caso – ela é falsa.\n",
      "3. Contradição: Mas se a sentença é falsa, então ela não é falsa conforme afirma, o que significa que a sentença seria verdadeira.\n",
      "4. Conclusão: A sentença não pode ser nem verdadeira nem falsa sem entrar em contradição, criando um paradoxo.\n",
      "Num tokens completion: 122\n",
      "Num tokens prompt: 63\n",
      "===========================================================\n",
      "\n",
      "Model: gpt-4-32k\n",
      "Elapsed time: 20.211388198999884\n",
      "Response: Paradoxo do gato de Schrodinger:\n",
      "\n",
      "O famoso físico Erwin Schrödinger propôs um experimento mental: Coloca-se um gato em uma caixa fechada com um dispositivo que tem 50% de chance de liberar um veneno mortal. Segundo a mecânica quântica, até que alguém abra a caixa para verificar, o gato está tanto vivo quanto morto simultaneamente. Quando a caixa é aberta, o gato é observado em um estado definido: vivo ou morto. Este paradoxo é usado para destacar as peculiaridades e limitações da interpretação de Copenhague da mecânica quântica.\n",
      "\n",
      "Num tokens completion: 155\n",
      "Num tokens prompt: 63\n",
      "===========================================================\n",
      "\n",
      "Model: gpt-4\n",
      "Elapsed time: 29.60398356299993\n",
      "Response: O \"Paradoxo do Barbeiro\" é um exemplo clássico de paradoxo, desenvolvido pelo matemático inglês Bertrand Russell. O paradoxo é enunciado da seguinte maneira:\n",
      "\n",
      "1. Em uma aldeia, existe um barbeiro que segue a regra: ele barbeia todos e apenas aqueles moradores que não se barbeiam.\n",
      "2. O paradoxo surge quando fazemos a pergunta: \"O barbeiro se barbeia?\"\n",
      "\n",
      "Se o barbeiro se barbeia, então ele viola a regra \"ele barbeia apenas aqueles que não se barbeiam a si mesmos\", pois ele se barbeia. Se o barbeiro não se barbeia, ele também viola a regra, porque a regra afirma que ele deve barbear todos que não se barbeiam, e isso incluiria ele mesmo. \n",
      "\n",
      "Logo, o barbeiro não pode nem não se barbear nem se barbear sem violar a regra, criando um paradoxo.\n",
      "Num tokens completion: 227\n",
      "Num tokens prompt: 63\n",
      "===========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_message = papel[\"instrutor\"]\n",
    "question = \"Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.\"\n",
    "models = [GPT_35_TURBO, GPT_4, GPT_4_32K, GPT_4_TURBO]\n",
    "results = execute_simultaneously(models, system_message, question, True)\n",
    "print_results(system_message, question, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E aí, qual o seu prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faça uma pergunta ao modelo, seja criativo!\n",
    "\n",
    "\n",
    "prompt = ''' \n",
    "COLOQUE SEU PROMPT AQUI\n",
    "'''\n",
    "\n",
    "system_message = papel[\"professor\"]\n",
    "\n",
    "response = call_llm(GPT_4_32K, system_message, prompt)\n",
    "response[\"2.response\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
