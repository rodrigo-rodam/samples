{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# install libraries\n",
    "%pip install --upgrade tiktoken\n",
    "%pip install --upgrade openai\n",
    "%pip install --upgrade python-dotenv\n",
    "%pip install --upgrade pydub\n",
    "%pip install --upgrade soundfile\n",
    "%pip install --upgrade openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pprint\n",
    "import tiktoken\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções que serão utilizadas durante os exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome das regiões Azure OpenAI\n",
    "EASTUS = \"eastus\"\n",
    "CANADAEAST = \"canadaeast\"\n",
    "NORTHCENTRALUS = \"northcentralus\"\n",
    "\n",
    "# Nome dos modelos Azure OpenAI criados na subscrição \n",
    "GPT_35_TURBO = \"gpt-35-turbo\"\n",
    "GPT_35_TURBO_16K = \"gpt-35-turbo-16k\"\n",
    "GPT_35_TURBO_INSTRUCT = \"gpt-35-turbo-instruct\"\n",
    "\n",
    "GPT_4 = \"gpt-4\"\n",
    "GPT_4_32K = \"gpt-4-32k\"\n",
    "GPT_4_TURBO = \"gpt-4-turbo\"\n",
    "GPT_4o = \"gpt-4o\"\n",
    "GPT_4o_mini = \"gpt-4o-mini\"\n",
    "\n",
    "DALL_E_3 = \"dall-e-3\"\n",
    "WHISPER = \"whisper\"\n",
    "TEXT_EMBEDDING_ADA_002 = \"text-embedding-ada-002\"\n",
    "TEXT_EMBEDDING_ADA_3_SMALL = \"text-embedding-3-small\"\n",
    "TEXT_EMBEDDING_ADA_3_LARGE = \"text-embedding-3-large\"\n",
    "\n",
    "# Whisper limit 25MB files\n",
    "audio_chunk_size_kb = 1024 * 22\n",
    "\n",
    "\n",
    "load_dotenv() # carregar variáveis de ambiente\n",
    "\n",
    "deployments_in_regions = {\"CANADAEAST\": [GPT_35_TURBO, GPT_4, GPT_4_32K, GPT_4_TURBO, TEXT_EMBEDDING_ADA_002, TEXT_EMBEDDING_ADA_3_SMALL],\n",
    "                          \"EASTUS\": [GPT_35_TURBO_16K, GPT_35_TURBO_INSTRUCT, GPT_4o, GPT_4o_mini, DALL_E_3, TEXT_EMBEDDING_ADA_002, TEXT_EMBEDDING_ADA_3_LARGE],\n",
    "                          \"NORTHCENTRALUS\": [WHISPER, GPT_4o]}\n",
    "\n",
    "\n",
    "# carregar tokenizador para os modelos de linguagem\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Exemplos de system messages\n",
    "papel = {\"bot\": \"Aja como um assistente virtual, capaz de responder perguntas, fornecer informações e que sempre responde em formato JSON.\",\n",
    "        \"escritor\": \"Aja como um especialista em escrita e redação, capaz de escrever, compreender e analisar textos em português, especialmente textos opinativos e argumentativos.\",\n",
    "        \"repentista\": \"Aja como um repentista, capaz de improvisar versos e rimas em português, especialmente em forma de sextilhas e martelos agalopados.\",\n",
    "        \"legislador\": \"Aja como um especialista em legislação, capaz de escrever, interpretar e analisar leis e normas jurídicas em português.\",\n",
    "        \"comediante\": \"Aja como um comediante, capaz de criar piadas, contar histórias engraçadas e fazer humor em português.\",\n",
    "        \"desevolvedor\": \"Aja como um desenvolvedor de software, capaz de criar, testar e manter programas de computador em qualquer linguagem, mas em especial em Python, HTML e Javascript\",\n",
    "        \"hacker\": \"Aja como um hacker, capaz de invadir sistemas de computadores, roubar informações e burlar sistemas de segurança.\",\n",
    "        \"professor\": \"Aja como um professor, capaz de ensinar e explicar conceitos, teorias e práticas em qualquer disciplina. Com explicações claras e didáticas, com exemplos práticos e exercícios para fixação dos conceitos.\",\n",
    "        \"instrutor\": \"Aja como um instrutor, capaz de ensinar e treinar pessoas em qualquer área de conhecimento. Suas explicações são diretas, sucintas e sem rodeios.\",\n",
    "        \"medico\": \"Aja como um médico, capaz de diagnosticar, tratar e prevenir doenças em seres humanos e animais.\",\n",
    "        \"advogado\": \"Aja como um advogado, capaz de representar clientes em processos judiciais, elaborar contratos e prestar consultoria jurídica.\",\n",
    "        \"cartomante\": \"Aja como um cartomante, capaz de ler cartas de tarô, búzios e outros oráculos, interpretar sonhos e prever o futuro e o destino das pessoas.\",\n",
    "        \"astrólogo\": \"Aja como um astrólogo, capaz de interpretar mapas astrais, prever o futuro e o destino das pessoas com base na posição dos astros e dos signos do zodíaco.\",\n",
    "        \"marketeiro\": \"Aja como um especialista em marketing, profundo conhecedor de branding, construção de marcas, propaganda e publicidade.\",\n",
    "        \"padre\": \"Aja como um padre, capaz de ministrar sacramentos, celebrar missas e aconselhar fiéis em questões espirituais.\",\n",
    "        \"rabino\": \"Aja como um rabino, capaz de ministrar cerimônias religiosas, ensinar a Torá e aconselhar fiéis em questões espirituais.\",\n",
    "        \"pastor\": \"Aja como um pastor, capaz de ministrar cultos, pregar a Bíblia e aconselhar fiéis em questões espirituais.\",}\n",
    "\n",
    "\n",
    "\n",
    "# inicialização do cliente Azure OpenAI Canada East\n",
    "client_canadaeast = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_CANADAEAST\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY_CANADAEAST\"),  \n",
    "    api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "# inicialização do cliente Azure OpenAI East US\n",
    "client_eastus = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_EASTUS\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY_EASTUS\"),  \n",
    "    api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "# inicialização do cliente Azure OpenAI North Central US\n",
    "client_northcentralus = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_NORTHCENTRALUS\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY_NORTHCENTRALUS\"),  \n",
    "    api_version=os.getenv(\"WHISPER_VERSION\")\n",
    ")\n",
    "\n",
    "\n",
    "# funcao para retornar o cliente de acordo com a região\n",
    "def get_client(region):\n",
    "    if region == \"canadaeast\":\n",
    "        return client_canadaeast\n",
    "    elif region == \"eastus\":\n",
    "        return client_eastus\n",
    "    elif region == \"northcentralus\":\n",
    "        return client_northcentralus\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# função para chamar o modelo de linguagem\n",
    "def call_llm(deployment_name, system_message, question, region=\"canadaeast\"):\n",
    "    #verificar se o modelo está disponível na região\n",
    "    if deployment_name not in deployments_in_regions[region.upper()]:\n",
    "        return {\"0.error\": f\"Model {deployment_name} not available in region {region}\", \n",
    "                \"1.This regions has the following models available\": deployments_in_regions[region.upper()], \n",
    "                f\"2.The model {deployment_name} is available in the following regions\": [region for region, models in deployments_in_regions.items() if deployment_name in models]}\n",
    "    \n",
    "    client = get_client(region)\n",
    "    start_time = time.perf_counter()\n",
    "    response = client.chat.completions.create(\n",
    "        model = deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "    )\n",
    "    elapsed_time = time.perf_counter() - start_time # usado para calcular o tempo de execução\n",
    "    resp = response.choices[0].message.content\n",
    "    tokens_completion = num_tokens_from_string(resp, deployment_name) # conta os tokens da resposta\n",
    "    # conta os tokens da mensagem do sistema e da pergunta (prompt)\n",
    "    tokens_prompt = num_tokens_from_string(system_message, deployment_name) + num_tokens_from_string(question, deployment_name)\n",
    "    # formatar a resposta da função\n",
    "    return {\"0.model\": deployment_name, \n",
    "            \"1.elapsed_time\": elapsed_time, \n",
    "            \"2.response\": resp, \n",
    "            \"3.num_tokens_completion\": tokens_completion, \n",
    "            \"4.num_tokens_prompt\": tokens_prompt,\n",
    "            \"5.system_message\": system_message,\n",
    "            \"6.question\": question}\n",
    "\n",
    "# funcao para contar tokens\n",
    "def num_tokens_from_string(texto, model):\n",
    "    if not texto:\n",
    "        return 0\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(texto))\n",
    "    return num_tokens\n",
    "\n",
    "# função para gerar embeddings\n",
    "def generate_embeddings(text, model, region=\"canadaeast\"):\n",
    "    client = get_client(region)\n",
    "    start_time = time.perf_counter()\n",
    "    embeddings = client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "    return embeddings, elapsed_time\n",
    "\n",
    "# função para executar modelos simultaneamente\n",
    "# models: lista de modelos\n",
    "# system_message: mensagem do sistema\n",
    "# question: pergunta\n",
    "# retorna uma lista com os resultados\n",
    "def execute_simultaneously(models, system_message, question, debug=False, region=\"canadaeast\"):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(call_llm, model, system_message, question, region) for model in models]\n",
    "        results = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            if debug:\n",
    "                print_result(future.result(), debug)\n",
    "            results.append(future.result())\n",
    "    return results\n",
    "\n",
    "def print_result(result, debug=False):\n",
    "    print(\"Elapsed time:\", result[\"1.elapsed_time\"])\n",
    "    print(\"Response:\", result[\"2.response\"])\n",
    "    print(\"Num tokens completion:\", result[\"3.num_tokens_completion\"])\n",
    "    print(\"Num tokens prompt:\", result[\"4.num_tokens_prompt\"])\n",
    "    if debug:\n",
    "        print(\"System message:\", result[\"5.system_message\"])\n",
    "        print(\"Question:\", result[\"6.question\"])\n",
    "        print(\"Model:\", result[\"0.model\"])\n",
    "        print(\"===========================================================\\n\")  \n",
    "\n",
    "#funcao para imprimir resultados\n",
    "def print_results(system_message, question, results, debug=False):\n",
    "    if debug:\n",
    "        print(\"System message:\", system_message)\n",
    "        print(\"Question:\", question)\n",
    "\n",
    "    for result in results:\n",
    "        print(\"Model:\", result[\"0.model\"], \"Elapsed time:\", result[\"1.elapsed_time\"])\n",
    "        print(\"Response:\", result[\"2.response\"])\n",
    "        print(\"Num tokens completion:\", result[\"3.num_tokens_completion\"], \n",
    "              \", Num tokens prompt:\", result[\"4.num_tokens_prompt\"])\n",
    "        print(\"===========================================================\\n\")\n",
    "\n",
    "\n",
    "def load_log_file(file_path):\n",
    "    # Definir um regex para capturar os campos do log\n",
    "    log_pattern = r'(\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\d{3}) (\\d+) \"(.*?)\" \"(.*?)\" (\\d+)'\n",
    "\n",
    "    # Listas para armazenar as informações extraídas\n",
    "    log_data = {\n",
    "        'ip': [],\n",
    "        'timestamp': [],\n",
    "        'request': [],\n",
    "        'status_code': [],\n",
    "        'size': [],\n",
    "        'referrer': [],\n",
    "        'user_agent': [],\n",
    "        'response_time': []\n",
    "    }\n",
    "\n",
    "    # Ler o arquivo linha por linha\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            match = re.match(log_pattern, line)\n",
    "            if match:\n",
    "                log_data['ip'].append(match.group(1))\n",
    "                log_data['timestamp'].append(match.group(2))\n",
    "                log_data['request'].append(match.group(3))\n",
    "                log_data['status_code'].append(int(match.group(4)))\n",
    "                log_data['size'].append(int(match.group(5)))\n",
    "                log_data['referrer'].append(match.group(6))\n",
    "                log_data['user_agent'].append(match.group(7))\n",
    "                log_data['response_time'].append(int(match.group(8)))\n",
    "\n",
    "    # Converter para DataFrame pandas\n",
    "    df = pd.DataFrame(log_data)\n",
    "    # Converter a coluna 'timestamp' para o tipo datetime, permitindo formatos mistos\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='mixed')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Exemplo de uso\n",
    "file_path = 'logfiles.log'  # Caminho para o arquivo de log\n",
    "df_logs = load_log_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2.333334900002228\n",
      "Response: {\n",
      "  \"próximo_horário_livre\": \"12/03/2022 15:00:00\"\n",
      "}\n",
      "Num tokens completion: 26\n",
      "Num tokens prompt: 183\n"
     ]
    }
   ],
   "source": [
    "def sample_agenda():\n",
    "    agenda = '''12/03/2022 12:00:00 Ocupado 12/03/2022 13:00:00 Ocupado 12/03/2022 14:00:00 Ocupado 12/03/2022 15:00:00 Livre 12/03/2022 16:00:00 Ocupado 12/03/2022 17:00:00 Livre\n",
    "        12/03/2022 18:00:00 Ocupado 12/03/2022 19:00:00 Livre 12/03/2022 20:00:00 Ocupado'''\n",
    "    return call_llm(GPT_35_TURBO, papel[\"bot\"], \n",
    "                    f\"Com base nessa agenda, qual o próximo horário livre? {agenda}\", \"canadaeast\")\n",
    "\n",
    "resposta = sample_agenda()\n",
    "print_result(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de diferentes tons na resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta com tom DESPOJADO:\n",
      "('Inteligência artificial é tipo quando máquinas conseguem aprender e realizar '\n",
      " 'tarefas que normalmente precisariam do pensamento humano. É como ensinar um '\n",
      " 'computador a reconhecer imagens, entender linguagem, ou até mesmo tomar '\n",
      " 'decisões por conta própria. IA pode ser usada em muitos lugares, tipo em '\n",
      " 'carros autônomos, assistentes virtuais, ou até na recomendação de produtos '\n",
      " 'em sites de compras. Resumindo, é quando as máquinas mostram um jeitinho de '\n",
      " 'pensar!')\n",
      "Resposta com tom FORMAL:\n",
      "('A inteligência artificial é o ramo da ciência da computação que se dedica ao '\n",
      " 'desenvolvimento de sistemas e técnicas capazes de simular o processo de '\n",
      " 'pensamento humano, realizando tarefas que exigem decisões baseadas em '\n",
      " 'raciocínio, aprendizado e percepção. Esses sistemas são projetados para '\n",
      " 'realizar funções que, de outra forma, exigiriam a intervenção humana, a '\n",
      " 'partir de algoritmos e modelos matemáticos que permitem a tomada de decisões '\n",
      " 'autônomas.')\n",
      "Resposta com tom INFORMAL:\n",
      "('Inteligência artificial, ou IA, é tipo quando as máquinas conseguem realizar '\n",
      " 'tarefas que normalmente precisariam de inteligência humana, tipo aprender, '\n",
      " 'raciocinar, resolver problemas, tomar decisões e até mesmo entender '\n",
      " 'linguagem natural. Isso acontece através de algoritmos e sistemas que '\n",
      " 'processam grande quantidade de dados e utilizam técnicas de aprendizado de '\n",
      " 'máquina. Ou seja, é como se as máquinas \"aprendessem\" a fazer coisas de '\n",
      " 'forma autônoma, sacou? É tipo ter um cérebro de máquina no meio de tantos '\n",
      " 'circuitos!')\n"
     ]
    }
   ],
   "source": [
    "def sample_system_message_intonation(question):\n",
    "    system_message = {\"despojado\":\"Você é um assitente que sempre responde em tom despojado, de forma didática e em porguguês, apesar de eventualmente citar palavras em inglês.\",\n",
    "                      \"formal\":\"Você é um assistente que sempre responde em tom formal, rebuscado, sem citar palavras em inglês.\",\n",
    "                      \"informal\":\"Você é um assistente que sempre responde em tom informal, de forma didática e em português, com gírias e expressões coloquiais.\"}\n",
    "    \n",
    "\n",
    "    for item in system_message:\n",
    "        print(f\"Resposta com tom {item.upper()}:\")\n",
    "        r = call_llm(GPT_35_TURBO, system_message[item], question, \"canadaeast\")\n",
    "        pprint.pprint(r[\"2.response\"])\n",
    "\n",
    "sample_system_message_intonation(\"O que é inteligência artificial?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de diferença entre modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = papel[\"instrutor\"]\n",
    "question = \"Conte-me um paradoxo de maneira estrudurada e sucinta com o mínimo de detalhes possível.\"\n",
    "models = [GPT_35_TURBO, GPT_4, GPT_4_32K, GPT_4_TURBO]\n",
    "results = execute_simultaneously(models, system_message, question, True)\n",
    "print_results(system_message, question, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faça uma pergunta ao modelo, seja criativo!\n",
    "system_message = papel[\"astrólogo\"]\n",
    "\n",
    "prompt = ''' \n",
    "\n",
    "Diga o horóscopo do signo do dia de hoje para o signo de Áries.\n",
    "\n",
    "'''\n",
    "\n",
    "print(call_llm(GPT_4o, system_message, prompt, EASTUS)[\"2.response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>request</th>\n",
       "      <th>status_code</th>\n",
       "      <th>size</th>\n",
       "      <th>referrer</th>\n",
       "      <th>user_agent</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107.160.167.84</td>\n",
       "      <td>2023-02-17 19:05:01.237</td>\n",
       "      <td>DELETE /usr HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>5028</td>\n",
       "      <td>-</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 10; ONEPLUS A6000)...</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.158.33.132</td>\n",
       "      <td>2023-02-17 19:05:01.802</td>\n",
       "      <td>GET /usr/login HTTP/1.0</td>\n",
       "      <td>500</td>\n",
       "      <td>4954</td>\n",
       "      <td>-</td>\n",
       "      <td>Mozilla/5.0 (Android 10; Mobile; rv:84.0) Geck...</td>\n",
       "      <td>2929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124.126.180.201</td>\n",
       "      <td>2023-02-17 19:05:01.820</td>\n",
       "      <td>POST /usr/admin HTTP/1.0</td>\n",
       "      <td>403</td>\n",
       "      <td>4970</td>\n",
       "      <td>http://davis.com/blog/wp-content/postslogin.html</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.168.19.42</td>\n",
       "      <td>2023-02-17 19:05:02.500</td>\n",
       "      <td>DELETE /usr/register HTTP/1.0</td>\n",
       "      <td>403</td>\n",
       "      <td>5055</td>\n",
       "      <td>-</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:8...</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.224.29.254</td>\n",
       "      <td>2023-02-17 19:05:02.768</td>\n",
       "      <td>POST /usr/admin/developer HTTP/1.0</td>\n",
       "      <td>304</td>\n",
       "      <td>4990</td>\n",
       "      <td>-</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...</td>\n",
       "      <td>4130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>152.193.37.56</td>\n",
       "      <td>2023-02-17 20:28:36.959</td>\n",
       "      <td>DELETE /usr/admin HTTP/1.0</td>\n",
       "      <td>404</td>\n",
       "      <td>5008</td>\n",
       "      <td>http://davis.com/blog/wp-content/postslogin.html</td>\n",
       "      <td>Mozilla/5.0 (iPhone; CPU iPhone OS 12_4_9 like...</td>\n",
       "      <td>3775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>200.209.229.252</td>\n",
       "      <td>2023-02-17 20:28:37.569</td>\n",
       "      <td>DELETE /usr HTTP/1.0</td>\n",
       "      <td>502</td>\n",
       "      <td>4951</td>\n",
       "      <td>-</td>\n",
       "      <td>Mozilla/5.0 (iPhone; CPU iPhone OS 12_4_9 like...</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>223.58.153.52</td>\n",
       "      <td>2023-02-17 20:28:37.774</td>\n",
       "      <td>PUT /usr/admin HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>4968</td>\n",
       "      <td>-</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 10; ONEPLUS A6000)...</td>\n",
       "      <td>1633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>201.232.185.137</td>\n",
       "      <td>2023-02-17 20:28:38.766</td>\n",
       "      <td>PUT /usr/admin/developer HTTP/1.0</td>\n",
       "      <td>502</td>\n",
       "      <td>5062</td>\n",
       "      <td>-</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 10; ONEPLUS A6000)...</td>\n",
       "      <td>3009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9.146.207.248</td>\n",
       "      <td>2023-02-17 20:28:38.897</td>\n",
       "      <td>PUT /usr HTTP/1.0</td>\n",
       "      <td>502</td>\n",
       "      <td>5011</td>\n",
       "      <td>-</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 10; ONEPLUS A6000)...</td>\n",
       "      <td>4504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ip               timestamp  \\\n",
       "0      107.160.167.84 2023-02-17 19:05:01.237   \n",
       "1       51.158.33.132 2023-02-17 19:05:01.802   \n",
       "2     124.126.180.201 2023-02-17 19:05:01.820   \n",
       "3        43.168.19.42 2023-02-17 19:05:02.500   \n",
       "4       88.224.29.254 2023-02-17 19:05:02.768   \n",
       "...               ...                     ...   \n",
       "9995    152.193.37.56 2023-02-17 20:28:36.959   \n",
       "9996  200.209.229.252 2023-02-17 20:28:37.569   \n",
       "9997    223.58.153.52 2023-02-17 20:28:37.774   \n",
       "9998  201.232.185.137 2023-02-17 20:28:38.766   \n",
       "9999    9.146.207.248 2023-02-17 20:28:38.897   \n",
       "\n",
       "                                 request  status_code  size  \\\n",
       "0                   DELETE /usr HTTP/1.0          200  5028   \n",
       "1                GET /usr/login HTTP/1.0          500  4954   \n",
       "2               POST /usr/admin HTTP/1.0          403  4970   \n",
       "3          DELETE /usr/register HTTP/1.0          403  5055   \n",
       "4     POST /usr/admin/developer HTTP/1.0          304  4990   \n",
       "...                                  ...          ...   ...   \n",
       "9995          DELETE /usr/admin HTTP/1.0          404  5008   \n",
       "9996                DELETE /usr HTTP/1.0          502  4951   \n",
       "9997             PUT /usr/admin HTTP/1.0          200  4968   \n",
       "9998   PUT /usr/admin/developer HTTP/1.0          502  5062   \n",
       "9999                   PUT /usr HTTP/1.0          502  5011   \n",
       "\n",
       "                                              referrer  \\\n",
       "0                                                    -   \n",
       "1                                                    -   \n",
       "2     http://davis.com/blog/wp-content/postslogin.html   \n",
       "3                                                    -   \n",
       "4                                                    -   \n",
       "...                                                ...   \n",
       "9995  http://davis.com/blog/wp-content/postslogin.html   \n",
       "9996                                                 -   \n",
       "9997                                                 -   \n",
       "9998                                                 -   \n",
       "9999                                                 -   \n",
       "\n",
       "                                             user_agent  response_time  \n",
       "0     Mozilla/5.0 (Linux; Android 10; ONEPLUS A6000)...            965  \n",
       "1     Mozilla/5.0 (Android 10; Mobile; rv:84.0) Geck...           2929  \n",
       "2     Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...           1962  \n",
       "3     Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:8...            702  \n",
       "4     Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...           4130  \n",
       "...                                                 ...            ...  \n",
       "9995  Mozilla/5.0 (iPhone; CPU iPhone OS 12_4_9 like...           3775  \n",
       "9996  Mozilla/5.0 (iPhone; CPU iPhone OS 12_4_9 like...            590  \n",
       "9997  Mozilla/5.0 (Linux; Android 10; ONEPLUS A6000)...           1633  \n",
       "9998  Mozilla/5.0 (Linux; Android 10; ONEPLUS A6000)...           3009  \n",
       "9999  Mozilla/5.0 (Linux; Android 10; ONEPLUS A6000)...           4504  \n",
       "\n",
       "[10000 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'logfiles.log'  # Caminho para o arquivo de log\n",
    "df_logs = load_log_file(file_path)\n",
    "df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200- Ok, 303 - See Other, 304 - Not Modified, 403 - Forbidden , 404 - Not Found, 500 - Internal Server Error, 502 - Bad Gateway, \n",
    "\n",
    "df_selected = df_logs[['status_code', 'response_time']]\n",
    "df_as_csv = df_selected.to_csv()\n",
    "\n",
    "prompt = f'''\n",
    "com base no dataset fornecido a seguir, quais são os insights que você pode fornecer?\n",
    "\n",
    "Dataset:\n",
    "\n",
    "{df_as_csv}\n",
    "\n",
    "'''\n",
    "system_message = \"Aja como um assistente virtual, capaz de responder perguntas, fornecer informações sobre datasets de logs e ofercer insights sobre os dados.\"\n",
    "\n",
    "r = call_llm(GPT_4_TURBO, system_message, prompt, CANADAEAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Este dataset contém uma lista de registros de resposta a solicitações HTTP com os campos de código de status (status_code), e tempo de resposta (response_time) em milissegundos. Com base nisso, aqui estão alguns insights que podem ser observados nos dados:\n",
      "\n",
      "1. **Distribuição dos Códigos de Estado (status_code)**:\n",
      "   - Podemos verificar a frequência de cada tipo de código de status para entender como o servidor está respondendo. Por exemplo, um alto número de códigos de status na faixa de 500 pode indicar problemas do lado do servidor, enquanto muitos códigos 404 podem indicar muitos links quebrados ou erros no cliente.\n",
      "\n",
      "2. **Tempo Médio de Resposta**:\n",
      "   - Calcular o tempo médio de resposta pode ajudar a entender a eficiência do servidor. Tempos de resposta elevados podem ser um indicativo de problemas de desempenho.\n",
      "\n",
      "3. **Análise de Outliers de Tempo de Resposta**:\n",
      "   - Identificar outliers, ou seja, tempos de resposta excepcionalmente altos ou baixos, pode ajudar a identificar possíveis problemas de desempenho pontuais, seja no servidor ou na rede.\n",
      "\n",
      "4. **Correlações**:\n",
      "   - Analisar as correlações entre códigos de status e tempos de resposta pode indicar se determinados tipos de respostas tendem a levar mais tempo do que outros.\n",
      "\n",
      "5. **Padrões Temporais**:\n",
      "   - Se os timestamps estivessem disponíveis, poderíamos analisar os padrões de tempo, como horários de pico de tráfego e sua relação com tempo de resposta ou erros.\n",
      "\n",
      "6. **Análise de Tendência**:\n",
      "   - Com uma série temporária, poderíamos verificar se houve melhora ou piora nos tempos de resposta ou na frequência de códigos de erro ao longo do tempo.\n",
      "\n",
      "7. **Agrupamentos de Status**:\n",
      "   - Grupo de sucesso: Códigos iniciando com 2 indicam sucesso, como o 200 (OK).\n",
      "   - Redirecionamentos: Códigos começando com 3 indicam redirecionamento, como o 303 (See Other) e 304 (Not Modified).\n",
      "   - Erros do cliente: Códigos iniciando com 4 indicam erro do cliente, como o 404 (Not Found).\n",
      "   - Erros do servidor: Códigos iniciando com 5 indicam erro do servidor, como o 500 (Internal Server Error) e 502 (Bad Gateway)\n",
      "\n",
      "8. **Disponibilidade do Servidor**:\n",
      "   - A taxa de códigos de status de sucesso (2xx) em relação ao total pode indicar a disponibilidade geral do servidor. Quanto mais alta essa taxa, melhor a disponibilidade.\n",
      "\n",
      "Especificar uma análise mais aprofundada exigiria uma exploração detalhada desses dados, que poderiam ser realizados através de técnicas de estatística descritiva, visualização de dados e testes estatísticos, se necessário. Seria útil também ter mais informações relacionadas ao contexto desses logs, como quantas e quais solicitações estão sendo feitas, qual é o comportamento esperado, entre outros.\n"
     ]
    }
   ],
   "source": [
    "print(r[\"2.response\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
